# -*- coding: utf-8 -*-
"""Tweet_Classification

Automatically generated by Colaboratory.

Original file by Irina Malyugina is located at
    https://colab.research.google.com/drive/1AR19C_wUtVLLUo055zuHeM5Kpad1xFNl
"""

import math
import os
import numpy as np
from bs4 import BeautifulSoup as bs
import requests
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from torchtext.vocab import GloVe
import pandas as pd

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

troll_data = pd.read_csv("troll_tweets.csv", encoding='latin-1')
reg_tweet_data = pd.read_csv("real_tweets.csv")

# trimming troll_data so we have an equal number of examples for genuine and troll tweets
troll_data = troll_data.truncate(after=86459)

# 1 == troll tweet
troll_data['Type'] = np.ones(86460, dtype = int).tolist()
# 0 == genuine tweet
reg_tweet_data['Type'] = np.zeros(86460, dtype = int).tolist()

# removing unn columns
troll_data = troll_data.drop(['external_author_id', 'region',	'language', 'publish_date',	'harvested_date',	'following', 'followers',	'updates', 'post_type', 'account_type',	'new_june_2018',	'retweet',	'account_category'], 1)
reg_tweet_data = reg_tweet_data.drop(['Party'], 1)

# changing column names in reg_tweet_data so that the columns will match
reg_tweet_data.columns = ['author', 'content', 'Type']
# merging the dfs to create our entire body of data
total_data = pd.concat([reg_tweet_data, troll_data])

from sklearn.model_selection import train_test_split
X = total_data.drop(['Type'], 1)
y = total_data['Type']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

vectorizer = CountVectorizer(max_features=2000)

vectorizer.fit(X_train[['content']].values.ravel())

def vectorize(tweet_content, vectorizer):
  X = vectorizer.transform(tweet_content.ravel()).todense()
  print(X.shape)
  return X

print('\nPreparing train data...')
bow_train_X = vectorize(X_train[['content']].values, vectorizer)
bow_train_y = y_train

print('\nPreparing val data...')
bow_val_X = vectorize(X_test[['content']].values, vectorizer)
bow_val_y = y_test

def train_and_evaluate(train_X, train_y, val_X, val_y):
  
  model = LogisticRegression(solver='liblinear')
  model.fit(train_X, train_y) 

  train_y_pred = model.predict(train_X)
  print('Train accuracy', accuracy_score(train_y, train_y_pred))

  val_y_pred = model.predict(val_X)
  print('Val accuracy', accuracy_score(val_y, val_y_pred))

  print('Confusion matrix:')
  print(confusion_matrix(val_y, val_y_pred))

  prf = precision_recall_fscore_support(val_y, val_y_pred)

  print('Precision:', prf[0][1])
  print('Recall:', prf[1][1])
  print('F-Score:', prf[2][1])

  return model

train_and_evaluate(bow_train_X, bow_train_y, bow_val_X, bow_val_y)

VEC_SIZE = 300
glove = GloVe(name='6B', dim=VEC_SIZE)

# Returns word vector for word if it exists, else return None.
def get_word_vector(word):
    try:
      return glove.vectors[glove.stoi[word.lower()]].numpy()
    except KeyError:
      return None

def glove_transform(tweet_content):
    X = np.zeros((len(tweet_content), VEC_SIZE))
    for i, tweet_content in enumerate(tweet_content):
        found_words = 0.0
        tweet_content = tweet_content.strip()
        for word in tweet_content.split(): 
            vec = get_word_vector(word)
            if vec is not None:
                found_words += 1
                X[i] += vec

        if found_words > 0:
            X[i] /= found_words
            
    return X
  
glove_train_X = glove_transform(X_train[['content']].values.ravel())
glove_train_y = y_train

glove_val_X = glove_transform(X_test[['content']].values.ravel())
glove_val_y = y_test

train_and_evaluate(glove_train_X, glove_train_y, glove_val_X, glove_val_y)

train_data = pd.concat([X_train, y_train], 1)
test_data = pd.concat([X_test, y_test], 1)

def prepare_data(tweets, featurizer):
    X = []
    y = []
    data = tweets.to_numpy()
    for datapoint in data:
        text = datapoint[1].lower()
        y.append(datapoint[2])
        features = featurizer(text)
        feature_descriptions, feature_values = zip(*features.items())
        X.append(feature_values)
    return X, y, feature_descriptions
  
def get_normalized_count(text, phrase):
    return math.log(1 + text.count(phrase.lower()))

def keyword_featurizer(text):
    features = {}
    
    keywords = ['clinton', 'soros', 'liberal', 'antifa', 'mainstream', 'protesters', 'hillary', 'God', 'wiretap', 'riot', 'obama', 'troll', 'leftist']
    
    for keyword in keywords:
      features[keyword + ' keyword'] = get_normalized_count(text, keyword)
    
    return features


train_X, train_y, feature_descriptions = prepare_data(train_data, keyword_featurizer)
val_X, val_y, feature_descriptions = prepare_data(test_data, keyword_featurizer)

model_1 = train_and_evaluate(train_X, train_y, val_X, val_y)

sorted(zip(feature_descriptions, model_1.coef_[0].tolist()), key=lambda x: x[1])

def combine_features(X_list):
  return np.concatenate(X_list, axis=1)

train_X_list = [train_X, bow_train_X, glove_train_X]
val_X_list = [val_X, bow_val_X, glove_val_X]
combined_train_X = combine_features(train_X_list)
combined_val_X = combine_features(val_X_list)

model_2 = train_and_evaluate(combined_train_X, train_y, combined_val_X, val_y)
